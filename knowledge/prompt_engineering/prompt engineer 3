In the rapidly evolving artificial intelligence landscape, Large Language Models (LLMs), with OpenAI’s ChatGPT at the helm, have achieved remarkable prominence.

The technology demonstrates how the innovative use of language, coupled with computational power, can redefine human-machine interactions. The driving force behind this surge is ‘prompt engineering,’ an intricate process that involves crafting text prompts to effectively guide LLMs towards accurate task completion, eliminating the need for extra model training.

The effectiveness of Large Language Models (LLMs) can be greatly enhanced through carefully crafted prompts. These prompts play a crucial role in extracting superior performance and accuracy from language models. With well-designed prompts, LLMs can bring about transformative outcomes in both research and industrial applications. This enhanced proficiency enables LLMs to excel in a wide range of tasks, including complex question answering systems, arithmetic reasoning algorithms, and numerous others.

However, prompt engineering is not solely about crafting clever prompts. It is a multidimensional field that encompasses a wide range of skills and methodologies essential for the development of robust and effective LLMs and interaction with them. Prompt engineering involves incorporating safety measures, integrating domain-specific knowledge, and enhancing the performance of LLMs through the use of customized tools. These various aspects of prompt engineering are crucial for ensuring the reliability and effectiveness of LLMs in real-world applications.

With a growing interest in unlocking the full potential of LLMs, there is a pressing need for a comprehensive, technically nuanced guide to prompt engineering. In the following sections, we will delve into the core principles of prompting and explore advanced techniques for crafting effective prompts.

What is prompt engineering and what are its uses?
Importance of prompt engineering in Natural Language Processing (NLP) and artificial intelligence
Prompt categories
Prompt engineering techniques
N-shot prompting
Chain-of-thought (CoT) prompting
Generated knowledge prompting
Directional stimulus prompting
ReAct prompting
Multimodal CoT prompting
Graph prompting
Prompt engineering: The step-by-step process
Understanding the problem
Crafting the initial prompt
Evaluating the model’s response
Iterating and refining the prompt
Testing the prompt on different models
Scaling the prompt
Key elements of a prompt
How to design prompts?
A case study on importance of effective prompt
Prompt engineering best practices
Applications of prompt engineering
Program-aided Language Model (PAL)
Generating data
Generating code
Risks associated with prompting and solutions
Adversarial prompting
Factuality
Biases
What is prompt engineering and what are its uses?
Prompt engineering is the practice of designing and refining specific text prompts to guide transformer-based language models, such as Large Language Models (LLMs), in generating desired outputs. It involves crafting clear and specific instructions and allowing the model sufficient time to process information. By carefully engineering prompts, practitioners can harness the capabilities of LLMs to achieve different goals.

The process of prompt engineering entails analyzing data and task requirements, designing and refining prompts, and fine-tuning the language model based on these prompts. Adjustments to prompt parameters, such as length, complexity, format, and structure, are made to optimize model performance for the specific task at hand.

Professionals in the field of artificial intelligence, including researchers, data scientists, machine learning engineers, and natural language processing experts, utilize prompt engineering to improve the performance and capabilities of LLMs and other AI models. It has applications in various domains, such as improving customer experience in e-commerce, enhancing healthcare applications, and building better conversational AI systems.

Successful examples of prompt engineering include OpenAI’s GPT-3 model for translation and creative writing, Google’s Smart Reply feature for automated message responses, and DeepMind’s AlphaGo for playing the game of Go at a high level. In each case, carefully crafted prompts were used to train the models and guide their outputs to achieve specific objectives.

Prompt engineering is crucial for controlling and guiding the outputs of LLMs, ensuring coherence, relevance, and accuracy in generated responses. It helps practitioners understand the limitations of the models and refine them accordingly, maximizing their potential while mitigating unwanted creative deviations or biases.

Isa Fulford and Andrew Ng outlined two principal aspects of prompt engineering in their ChatGPT Prompt Engineering for Developers course:

Formulation of clear and specific instructions: This principle stresses the importance of conciseness and specificity in the prompt construction. Clear prompts assist the model in precisely understanding the required task, thus leading to a more accurate and relevant output.
Allowing the model time to “Think”: This principle underscores the significance of giving the model enough time to process the given information. Incorporating pauses or “thinking time” in the prompts can help the model better process and interpret the input, leading to an improved output.
Given the complex nature of LLMs and their inherent tendency to ‘hallucinate,’ carefully designed and controlled prompts can help manage these occurrences. Prompt engineering, therefore, plays a crucial role in maximizing the potential of LLMs and mitigating any unwanted creative deviations.

What is prompt engineering and what are its uses
Prompts used for various AI tasks
This section provides examples of how prompts are used for different tasks and introduces key concepts relevant to advanced sections.

Task	Example Prompt	Possible Output
Text Summarization	Explain antibiotics.	Antibiotics are medications used to treat bacterial infections…
Information Extraction	Mention the large language model based product mentioned in the paragraph.	The large language model based product mentioned in the paragraph is ChatGPT.
Question Answering	Answer the question based on the context below.	It was approved to help prevent organ rejection after kidney transplants.
Text Classification	Classify the text into neutral, negative, or positive.	Neutral
Conversation	The following is a conversation with an AI research assistant.	Black holes are regions of spacetime where gravity is extremely strong…
Code Generation	Ask the user for their name and say “Hello.”	let name = prompt(“What is your name?”); console.log(Hello, ${name}!);
Reasoning	The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.	No, the odd numbers in this group add up to an odd number: 119.
Contact LeewayHertz’s prompt engineers today!
Enhance the performance of your AI models with our prompt engineering services

Learn more
These examples demonstrate how well-crafted prompts can be utilized for various AI tasks, ranging from text summarization and information extraction to question answering, text classification, conversation, code generation, and reasoning. By providing clear instructions and relevant context in the prompts, we can guide the language model to generate desired outputs.

Importance of prompt engineering in Natural Language Processing (NLP) and artificial intelligence
In the realm of Natural Language Processing (NLP) and Artificial Intelligence (AI), prompt engineering has rapidly emerged as an essential aspect due to its transformative role in optimizing and controlling the performance of language models.

Maximizing model efficiency: While current transformer-based language models like GPT-3 or Google’s PaLM 2 possess a high degree of intelligence, they are not inherently task-specific. As such, they need well-crafted prompts to effectively generate the desired outputs. An intelligently designed prompt ensures that the model’s capabilities are utilized optimally, leading to the production of relevant, accurate, and high-quality responses. Thus, prompt engineering allows developers to harness the full potential of these advanced models without the need for extensive re-training or fine-tuning.
Enhancing task-specific performance: The goal of AI is to enable machines to perform as closely as possible to, or even surpass, human levels. Prompt engineering enables AI models to provide more nuanced, context-aware responses, making them more efficient for specific tasks. Whether it’s language translation, sentiment analysis, or text generation, prompt engineering helps to align the model’s output with the task’s requirements.
Understanding model limitations: Working with prompts can provide insight into the limitations of a language model. Through iterative refining of prompts and studying the responses of the model, we can understand its strengths and weaknesses. This knowledge can guide future model development, feature enhancement, and even lead to new approaches in NLP.
Increasing model safety: AI safety is an important concern, especially when using language models for public-facing applications. A poorly designed prompt might lead the model to generate inappropriate or harmful content. Skilled prompt engineering can help prevent such issues, making the AI model safer to use.
Enabling resource efficiency: Training large language models demands considerable computational resources. However, with effective prompt engineering, developers can significantly improve the performance of a pre-trained model without additional resource-intensive training. This not only makes the AI development process more resource-efficient but also more accessible to those with limited computational resources.
Facilitating domain-specific knowledge transfer: Through skilled prompt engineering, developers can imbue language models with domain-specific knowledge, allowing them to perform more effectively in specialized fields such as medical, legal, or technical contexts.
In a nutshell, prompt engineering is crucial for the effective utilization of large language models in NLP-based and other tasks, helping to maximize model performance, ensure safety, conserve resources, and improve domain-specific outputs. As we move forward into an era where AI is increasingly integrated into daily life, the importance of this field will only continue to grow.

Prompt categories
Prompts play a crucial role in fostering efficient interaction with AI language models. The fundamental aspect of crafting proficient prompts lies in comprehending their diverse types. This comprehension greatly facilitates the process of tailoring prompts to elicit a specific desired response.

There are several primary categories of prompts:

Queries for information – These prompts are tailored to obtain information. They generally respond to the questions “What” and “How.” Instances of such prompts are: “What are the top places to visit in Kenya?”, “How can I prepare for a job interview?”
Task-specific prompts – They are employed to direct the model to accomplish a certain task. These types of prompts are commonly seen in use with Siri, Alexa, or Google Assistant. For instance, a task-specific prompt could be “Dial mom” or “Begin playing the newest episode of my preferred TV series.”
Context-supplying prompts – As the name implies, these prompts offer details to the AI to help it accurately discern the user’s requirements. For instance, if you’re organizing a party and need some inspiration for decorations and activities, you could structure your prompt like this: “I’m organizing a party for my child, could you suggest some decoration ideas and activities to make it fun and memorable?”
Comparative prompts – These are utilized to assess or compare different choices given to the model to assist the user in making an appropriate decision. For instance: “How does Option A stack up against Option B in terms of strengths and weaknesses?”
Opinion-eliciting prompts – These are crafted to solicit the AI’s viewpoint on a particular subject. For instance: “What might occur if time travel were possible?”
Reflective prompts – These prompts are designed to help individuals delve deeper into understanding themselves, their beliefs, and their actions. They are akin to prompts for self-improvement based on a topic or personal experience. A bit of prior information might be required to get the desired response.
Role-specific prompts – Role prompting is a technique where the model is instructed to take on a specific role before engaging in a particular task. It involves using a prompt to define the AI’s role, such as a doctor or lawyer, and then asking questions or providing scenarios related to that role. This is the most frequently employed category of prompts. Assigning a role to the AI produces responses based on that role. A useful strategy for this category is to employ the 5 Ws framework, which includes:
Who – Assigns the role you want the model to assume. A role such as a teacher, developer, chef, etc.
What – Refers to the action you want the model to perform.
When – Your desired timeframe for accomplishing a particular task.
Where – Refers to the location or context of a specific prompt.
Why – Refers to a specific prompt’s reasons, motives, or objectives.
Prompt engineering techniques
Prompt engineering, an emergent area of research that has seen considerable advancements since 2022, employs a number of novel techniques to enhance the performance of language models. Each of these techniques brings a unique approach to instructing large language models, highlighting the versatility and adaptability inherent in the field of prompt engineering. They form the foundation for effectively communicating with these models, shaping their output, and harnessing their capabilities to their fullest potential. Some of the most useful methods widely implemented in this field are:

N-shot prompting (zero-shot prompting and few-shot prompting)
The term “N-shot prompting” is used to represent a spectrum of approaches where N symbolizes the count of examples or cues given to the language model to assist in generating predictions. This spectrum includes, notably, zero-shot prompting and few-shot prompting.

Zero-shot prompting refers to a situation where the language model generates predictions without any explicit, additional examples. It’s particularly effective for tasks the model has been extensively trained on, including but not limited to, classification tasks like sentiment analysis or spam detection, text transformation tasks like translation or summarization, and simple text generation.
Let’s consider the task of sentiment analysis where the model is given a sentence and is expected to classify the sentiment as positive, negative, or neutral. In this case, we could directly ask the model without providing any explicit training examples. This would be an instance of zero-shot prompting. Here is how it might look:

Input prompt to the model:

"What is the sentiment of the following sentence: 'I had an amazing day at the park'?"
Given that the language model has been extensively trained on understanding sentiments during its initial training, it can typically respond to such a prompt accurately, despite not receiving specific examples in this context. The expected response would be:

Model’s response:

"The sentiment of the sentence is positive."
In this scenario, we haven’t provided any training examples or clues. We are directly asking the model to perform the task, hence it’s a zero-shot prompt.

On the other hand, few-shot prompting employs a limited set of examples, usually ranging from two to five, to guide the model’s output. These instances are designed to direct the model towards improved performance in addressing more context-specific problems. By offering a snapshot of the desired output, few-shot prompts enable the model to tailor its responses more effectively, thus increasing the accuracy of its predictions.
Let’s consider the task of generating a rhymed couplet (two-line verse) about a given topic, which is a more context-specific task and might benefit from few-shot prompting. Here is how it might look:

Input prompt to the model:

"Write a rhymed couplet about a sunflower:
Example 1:
'Sunflower with petals bright,
Basking gladly in the sunlight.'
Example 2:
'Sunflower tall in the summer glow,
Nodding as the breezes blow.'
 
Now, write a rhymed couplet about a moonlit night."
Here, we are providing the model with two examples of how to write a rhymed couplet about a specific topic, in this case, a sunflower. These examples serve as context and steer the model towards the desired output. They essentially “tell” the model what we expect from it.

The model, given its extensive training data and the two specific examples (few-shot prompting), is expected to generate a similar rhymed couplet about a moonlit night. The response could be:

Model’s response:

"Moonlight spreading its silver light,
Bathing the world in a tranquil night."
In this scenario, we have provided a few examples or clues before asking the model to perform the task, hence it’s a few-shot prompt.

Chain-of-thought (CoT) prompting
Chain-of-thought (CoT) prompting, a technique introduced by Google researchers, operates on the concept of encouraging an AI model to elucidate intermediate reasoning stages before delivering the final answer to a multi-stage issue. The objective is to design the model’s reasoning trajectory to resemble the intuitive cognitive process one would employ while tackling a complex problem involving multiple steps. This procedure allows the model to dissect intricate problems into simpler components, thereby enabling it to address challenging reasoning tasks that traditional prompting methods might not handle effectively.

Let’s consider a complex problem-solving example in which Chain-of-thought (CoT) prompting can be applied.

Consider a prompt where we want a language model to solve a multi-step math word problem like this:

"John has 10 apples. He gives 3 apples to his friend Sam and then buys 6 more apples from the market. How many apples does John have now?"
Using Chain-of-thought prompting, we would split the problem into simpler intermediate steps:

Initial Prompt: "John has 10 apples." Intermediate Prompt: "How many apples does John have if he gives 3 to Sam?" Intermediate Answer: "John has 7 apples."
 
Initial Prompt: "John has 7 apples." Intermediate Prompt: "How many apples will John have if he buys 6 more apples from the market?" Intermediate Answer: "John has 13 apples."
Finally, we have the answer to the original complex problem: “John has 13 apples now.”

The chain-of-thought prompting method breaks down the problem into manageable pieces, allowing the model to reason through each step and then build up to the final answer. This method helps to increase the model’s problem-solving capabilities and overall understanding of complex tasks.

There are several innovative adaptations of chain-of-thought prompting, including:

Self-consistency prompting: This variation involves creating multiple diverse paths of reasoning and selecting answers that show the highest level of consistency. This method ensures increased precision and dependability in answers by implementing a consensus-based system.
Let’s consider an example where we use self-consistency prompting in a scenario involving decision-making based on diverse reasoning paths.

Let’s assume we are asking a language model to predict the best holiday destination considering the following variables: budget, proximity, weather, and activities. The resulting question could be:

"What would be the best holiday destination considering a budget of $3000, a preference for warm weather, and a love for beach activities?"
We then sample diverse reasoning paths:

Path 1: Initial prompt: "Considering a budget of $3000 and a preference for warm weather." Intermediate answer: "A tropical destination would fit these criteria. Let's consider Bali."
 
Following prompt: "What about considering beach activities?" Final answer: "Bali is renowned for its beach activities. So, it could be the best choice."
 
Path 2: Initial prompt: "Considering a preference for warm weather and a love for beach activities." Intermediate answer: "Hawaii is well-known for its excellent weather and beach activities."
 
Following prompt: "Considering a budget of $3000, is Hawaii a good choice?" Final answer: "Yes, Hawaii could be a great choice considering all three factors."
 
Path 3: Initial prompt: "Considering a budget of $3000 and a love for beach activities." Intermediate answer: "Considering these factors, the Bahamas could be a great destination."
Following prompt:

"Considering a preference for warm weather, is the Bahamas a good choice?"
Final answer:

"Indeed, the Bahamas is known for its warm weather. So, it could be the best choice."
In this case, all paths led to valid holiday destinations. To make a decision, we then apply a majority voting system, wherein the most consistent answer would be chosen as the final output of the self-consistency prompting process. Given the diversity of the prompts, the most consistent destination can be considered the most suitable for the given conditions.

Least-to-most prompting (LtM): Here, the chain-of-thought technique begins by fragmenting a problem into a series of less complex sub-problems. The model then solves them in an ordered sequence. Each subsequent sub-problem is solved using the solutions to previously addressed sub-problems. This methodology is motivated by real-world teaching strategies used in educating children.
Let’s consider an example where we use least-to-most prompting in the context of solving a mathematical word problem. The problem is: “John has twice as many apples as Jane. Jane has 5 apples. How many apples does John have?”

In the least-to-most prompting approach, we would break down this problem into simpler subproblems and solve them sequentially.

First subproblem: Initial prompt:

"Jane has 5 apples." Intermediate answer: "So, the number of apples Jane has is 5."
Second Subproblem: Initial prompt:

"John has twice as many apples as Jane."
Intermediate answer:

"So, John has 2 times the number of apples that Jane has."
Third Subproblem: Initial prompt:

"Given that Jane has 5 apples and John has twice as many apples as Jane, how many apples does John have?"
Final answer:

"John has 2 * 5 = 10 apples."
In this way, the least-to-most prompting technique decomposes a complex problem into simpler subproblems and builds upon the answers to previously solved subproblems to arrive at the final answer.

Active prompting: This technique scales the CoT approach by identifying the most crucial and beneficial questions for human annotation. Initially, the model computes the uncertainty present in the LLM’s predictions, then it selects the questions that contain the highest uncertainty. These questions are sent for human annotation, after which they are integrated into a CoT prompt.
Active prompting involves identifying and selecting uncertain questions for human annotation. Let’s consider an example from the perspective of a language model engaged in a conversation about climate change.

Let’s assume our model has identified three potential questions that could be generated from its current conversation, with varying levels of uncertainty:

What is the average global temperature?
What are the primary causes of global warming?
How does carbon dioxide contribute to the greenhouse effect?
In this scenario, the model might be relatively confident about the answers to the first two questions, since these are common questions about the topic. However, it might be less certain about the specifics of how carbon dioxide contributes to the greenhouse effect.

Active prompting would identify the third question as the most uncertain, and thus most valuable for human annotation. After this question is selected, a human would provide the model with the information required to correctly answer the question. The annotated question and answer would then be added to the model’s prompt, enabling it to better handle similar questions in the future.

Contact LeewayHertz’s prompt engineers today!
Enhance the performance of your AI models with our prompt engineering services

Learn more
Generated knowledge prompting
Generated knowledge prompting operates on the principle of leveraging a large language model’s ability to produce potentially beneficial information related to a given prompt. The concept is to let the language model offer additional knowledge which can then be used to shape a more informed, contextual, and precise final response.

For instance, if we are using a language model to provide answers to complex technical questions, we might first use a prompt that asks the model to generate an overview or explanation of the topic related to the question.

Suppose the question is: “Can you explain how quantum entanglement works in quantum computing?”

We might first prompt the model with a question like, “Provide an overview of quantum entanglement.” The model might generate a response detailing the basics of quantum entanglement.

We would then use this generated knowledge as part of our next prompt. We might ask: “Given that quantum entanglement involves the instantaneous connection between two particles regardless of distance, how does this concept apply in quantum computing?”

By using generated knowledge prompting in this way, we are able to facilitate more informed, accurate, and contextually aware responses from the language model.

Directional stimulus prompting
Directional stimulus prompting is another advanced technique in the field of prompt engineering where the aim is to direct the language model’s response in a specific manner. This technique can be particularly useful when you are seeking an output that has a certain format, structure, or tone.

For instance, suppose you want the model to generate a concise summary of a given text. Using a directional stimulus prompt, you might specify not only the task (“summarize this text”) but also the desired outcome, by adding additional instructions such as “in one sentence” or “in less than 50 words”. This helps to direct the model towards generating a summary that aligns with your requirements.

Here is an example: Given a news article about a new product launch, instead of asking the model “Summarize this article,” you might use a directional stimulus prompt such as “Summarize this article in a single sentence that could be used as a headline.”

Another example could be in generating rhymes. Instead of asking, “Generate a rhyme,” a directional stimulus prompt might be, “Generate a rhyme in the style of Dr. Seuss about friendship.”

By providing clear, specific instructions within the prompt, directional stimulus prompting helps guide the language model to generate output that aligns closely with your specific needs and preferences.

ReAct prompting
ReAct prompting is a technique inspired by the way humans learn new tasks and make decisions through a combination of “reasoning” and “acting”. This innovative methodology seeks to address the limitations of previous methods like Chain-of-thought (CoT) prompting, which, despite its ability to generate reasonable answers for various tasks, has issues related to fact hallucination and error propagation due to its lack of interaction with external environments and inability to update its knowledge.

ReAct prompting pushes the boundaries of large language models by prompting them to not only generate verbal reasoning traces but also actions related to the task at hand. This hybrid approach enables the model to dynamically reason and adapt its plans while interacting with external environments, such as databases, APIs, or in simpler cases, information-rich sites like Wikipedia.

For example, if we task an LLM with the goal of creating a detailed report on the current state of artificial intelligence, using ReAct prompting, the model would not just generate responses based on its pre-existing knowledge. Instead, it would plan a sequence of actions, such as fetching the latest AI research papers from a database or querying for recent news on AI from reputable sources. It would then integrate this up-to-date information into its reasoning process, resulting in a more accurate and comprehensive report. This two-pronged approach of acting and reasoning can mitigate the limitations observed in prior prompting methods and empower LLMs with enhanced accuracy and depth.

Consider a scenario where a user wants to know the current state of a particular stock. Using the ReAct prompting technique, the task might unfold in the following steps:

Step 1 (Reasoning): The LLM determines that to fulfill this request, it needs to fetch the most recent stock information. The model identifies the required action, i.e., accessing the latest stock data from a reliable financial database or API.
Step 2 (Acting): The model generates a command to retrieve the data: “Fetch latest stock data for ‘Company X’ from the Financial Database API”.
Step 3 (Interaction): The command is executed, and the model receives the up-to-date stock information.
Step 4 (Reasoning and Acting): With the latest stock data now available, the model processes this information and generates a detailed response: “As of today, the stock price of ‘Company X’ is at $Y, which represents a Z% increase from last week.”
In this example, the LLM demonstrates its ability to reason and generate actions (fetching the data), interact with an external environment (the financial database API), and ultimately provide a precise and informed response based on the most recent data available.

Multimodal CoT prompting
Multimodal CoT prompting is an extension of the original CoT prompting, involving multiple modes of data, usually both text and images. By using this technique, a large language model can leverage visual information in addition to text to generate more accurate and contextually relevant responses. This allows the system to carry out more complex reasoning that involves both visual and textual data.

For instance, consider a scenario where a user wants to know the type of bird shown in a particular image. Using the multimodal CoT prompting technique, the task might unfold as follows:

Step 1 (Reasoning): The LLM recognizes that it needs to identify the bird in the image. However, instead of making a direct guess, it decides to carry out a sequence of reasoning steps, first trying to identify the distinguishing features of the bird.
Step 2 (Acting): The model generates a command to analyze the image: “Analyze the bird’s features in the image, such as color, size, and beak shape.”
Step 3 (Interaction): The command is executed, and the model receives the visual analysis of the bird: “The bird has blue feathers, a small body, and a pointed beak.”
Step 4 (Reasoning and Acting): With these distinguishing features now available, the model cross-references this information with its textual knowledge about bird species. It concludes that the bird is likely to be a “Blue Tit.”
Step 5 (Final Response): The model provides its final answer: “Based on the blue feathers, small body, and pointed beak, the bird in the image appears to be a Blue Tit.”
In this example, multimodal CoT prompting allows the LLM to generate a chain of reasoning that involves both image analysis and textual cross-referencing, leading to a more informed and accurate answer.

Graph prompting
Graph prompting is a method for leveraging the structure and content of a graph for prompting a large language model. In graph prompting, you use a graph as the primary source of information and then translate that information into a format that can be understood and processed by the LLM. The graph could represent many types of relationships, including social networks, biological pathways, and organizational hierarchies, among others.

For example, let us consider a graph that represents relationships between individuals in a social network. The nodes of the graph represent people, and the edges represent relationships between them. Let us say you want to find out who in the network has the most connections.

You would start by translating the graph into a textual description that an LLM can process. This could be a list of relationships like “Alice is friends with Bob,” “Bob is friends with Charlie,” “Alice is friends with Charlie,” and so on.

Next, you would craft a prompt that asks the LLM to analyze these relationships and identify the person with the most connections. The prompt might look like this: “Given the following list of friendships, who has the most friends: Alice is friends with Bob, Bob is friends with Charlie, Alice is friends with Charlie.”

The LLM would then process this prompt and provide an answer based on its analysis of the information. For instance, in this case, the answer might be “Alice”, given that she has the most connections according to the provided list of relationships.

Through graph prompting, you are essentially converting structured graph data into a text-based format that LLMs can understand and reason about, opening up new possibilities for question answering and problem solving.

Prompt engineering: The step-by-step process
Prompt engineering is a multi-step process that involves several key tasks. Here they are:

Understanding the problem
Understanding the problem is a critical first step in prompt engineering. It requires not just knowing what you want your model to do, but also understanding the underlying structure and nuances of the task at hand. This is where the art and science of problem analysis in the context of AI comes into play.

The type of problem you are dealing with greatly influences the approach you will take when crafting prompts. For instance:

Question-answering tasks: For a question-answering task, you would need to understand the type of information needed in the answer. Is it factual? Analytical? Subjective? Also, you would have to consider whether the answer requires reasoning or context.
Text generation tasks: If it is a text generation task, factors like the desired length of the output, its format (story, poem, article), and its tone or style come into play.
Sentiment analysis tasks: For sentiment analysis, the prompt should be structured to guide the model to recognize subjective expressions and discern the sentiment from the text.
Understanding the problem also involves identifying any potential challenges or limitations associated with the task. For instance, a task might involve domain-specific language, slang, or cultural references, which the model may or may not be familiar with.

Moreover, understanding the problem thoroughly helps in anticipating how the model might react to different prompts. You might need to provide explicit instructions, or use a specific format for the prompt. Or, you may need to iterate and refine the prompts several times to get the desired output.

Ultimately, a deep understanding of the problem allows for the creation of more effective and precise prompts, which in turn leads to better performance from the large language model.

Crafting the initial prompt
Crafting the initial prompt is an essential task in the process of prompt engineering. This step involves the careful composition of an initial set of instructions to guide the language model’s output, based on the understanding gained from the problem analysis.

The main objective of a prompt is to provide clear, concise, and unambiguous directives to the language model. It acts as a steering wheel, directing the model to the required path and desired output. A well-structured prompt can effectively utilize the capabilities of the model, producing high-quality and task-specific responses.

In some scenarios, especially in tasks that require a specific format or context-dependent results, the initial prompt may also incorporate a few examples of the desired inputs and outputs, known as few-shot examples. This method is often used to give the model a clearer understanding of the expected result.

For instance, if you want the model to translate English text into French, your prompt might include a few examples of English sentences and their corresponding French translations. This helps the model to grasp the pattern and the context better.

Remember, while crafting the initial prompt, it is also essential to maintain flexibility. The ideal output is seldom achieved with the first prompt attempt. Often, you would need to iterate and refine the prompts, based on the model’s responses, to achieve the desired results. This process of iterative refinement is an integral part of prompt engineering.

Evaluating the model’s response
Evaluating the model’s response is a crucial phase in prompt engineering that follows after the initial prompt has been utilized to generate a model response. This step is key in understanding the effectiveness of the crafted prompt and the language model’s interpretive capacity.

The first thing to assess is whether the model’s output aligns with the task’s intended goal. For example, if the task is about translating English sentences into Spanish, does the output correctly and accurately render the meaning in Spanish? Or if the task is to generate a summary of a lengthy article, does the output present a concise and coherent overview of the article’s content?

When the model’s response does not meet the desired objective, it’s essential to identify the areas of discrepancy. This could be in terms of relevance, accuracy, completeness, or contextual understanding. For instance, the model might produce a grammatically correct sentence that is contextually incorrect or irrelevant.

Upon identifying the gaps, the aim should be to understand why the model is producing such output. Is the prompt not explicit enough? Or is the task too complex for the model’s existing capabilities? Answering these questions can provide insights into the limitations of the model as well as the prompt, guiding the next step in the prompt engineering process – Refining the prompts.

Evaluating the model’s response is a crucial iterative process in prompt engineering, acting as a feedback loop that consistently informs and improves the process of crafting more effective prompts.

Iterating and refining the prompt
Iterating and refining the prompt is an essential step in prompt engineering that arises from the evaluations of the model’s response. This stage centers on improving the effectiveness of the prompt based on the identified shortcomings or flaws in the model’s output.

When refining a prompt, several strategies can be employed. These strategies are predominantly influenced by the nature of the misalignment between the model’s output and the desired objective.

For instance, if the model’s response deviates from the task’s goal due to a lack of explicit instructions in the prompt, the refinement process may involve making the instructions clearer and more specific. Explicit instructions help ensure that the model comprehends the intended objective and doesn’t deviate into unrelated content or produce irrelevant responses.

On the other hand, if the model is struggling to understand the structure of the task or the required output, it may be beneficial to provide more examples within the prompt. These examples can act as guidelines, demonstrating the correct form and substance of the desired output.

Similarly, the format or structure of the prompt itself can be altered in the refinement process. The alterations could range from changing the order of sentences or the phrasing of questions to the inclusion of specific keywords or format cues.

The iteration and refinement process in prompt engineering is cyclic, with multiple rounds of refinements often necessary to arrive at a prompt that most effectively elicits the desired output from the model. It is a process that underlines the essence of prompt engineering – the fine-tuning of language to communicate effectively with large language models.

Testing the prompt on different models
Testing the prompt on different models is a significant step in prompt engineering that can provide in-depth insights into the robustness and generalizability of the refined prompt. This step entails applying your prompt to a variety of large language models and observing their responses. It is essential to understand that while a prompt may work effectively with one model, it may not yield the desired result when applied to another. This is because different models may have different architectures, training methodologies, or datasets that influence their understanding and response to a particular prompt.

The size of the model plays a significant role in its ability to understand and respond accurately to a prompt. For instance, larger models often have a broader context window and can generate more nuanced responses. On the other hand, smaller models may require more explicit prompting due to their reduced contextual understanding.

The model’s architecture, such as transformer-based models like GPT-3 or LSTM-based models, can also influence how it processes and responds to prompts. Some architectures may excel at certain tasks, while others may struggle, and this can be unveiled during this testing phase.

Lastly, the training data of the models plays a crucial role in their performance. A model trained on a wide range of topics and genres may provide a more versatile response than a model trained on a narrow, specialized dataset.

By testing your prompt across various models, you can gain insights into the robustness of your prompt, understand how different model characteristics influence the response, and further refine your prompt if necessary. This process ultimately ensures that your prompt is as effective and versatile as possible, reinforcing the applicability of prompt engineering across different large language models.

Scaling the prompt
After refining and testing your prompt to a point where it consistently produces desirable results, it’s time to scale it. Scaling, in the context of prompt engineering, involves extending the utility of a successfully implemented prompt across broader contexts, tasks, or automation levels.

Automating prompt generation: Depending on the nature of the task and the model’s requirements, it may be possible to automate the process of generating prompts. This could involve creating a script or a tool that generates prompts based on certain parameters or rules. Automating prompt generation can save a significant amount of time, especially when dealing with a high volume of tasks or data. It can also reduce the chance of human error and ensure consistency in the prompt generation process.
Creating variations of the prompt: Another way to scale a prompt is to create variations that can be used for related tasks. For example, if you have a prompt that successfully guides a model in performing sentiment analysis on product reviews, you might create variations of this prompt to apply it to movie reviews, book reviews, or restaurant reviews. This approach leverages the foundational work that went into creating the original prompt and allows you to address a wider range of tasks more quickly and efficiently.
Scaling the prompt is the final step in the prompt engineering process, reflecting the successful development of an effective prompt. It represents a transition from development to deployment, as the prompt begins to be used in real-world applications on a broader scale.

It’s worth noting that prompt engineering is an iterative process. It requires ongoing testing and refinement to optimize the model’s performance for the given task.

Key elements of a prompt
Delving into the world of prompt engineering, we encounter four pivotal components that together form the cornerstone of this discipline. These are instructions, context, input data, and output indicators. Together, they provide a framework for effective communication with large language models, shaping their responses and guiding their operations. Here, we explore each of these elements in depth, helping you comprehend and apply them efficiently in your AI development journey.

Instruction: This is the directive given to the model that details what is expected in terms of the task to be performed. This could range from “translate the following text into French” to “generate a list of ideas for a science fiction story”. The instruction is usually the first part of the prompt and sets the overall task for the model.
Context: This element provides additional information that can guide the model’s response. For instance, in a translation task, you might provide some background on the text to be translated (like it’s a dialogue from a film or a passage from a scientific paper). The context can help the model understand the style, tone, and specifics of the information needed.
Input data: This refers to the actual data that the model will be working with. In a translation task, this would be the text to be translated. In a question-answering task, this would be the question being asked.
Output indicator: This part of the prompt signals to the model the format in which the output should be generated. For instance, you might specify that you want the model’s response in the form of a list, a paragraph, a single sentence, or any other specific structure. This can help narrow down the model’s output and guide it towards more useful responses.
While these elements are not always required in every prompt, a well-crafted prompt often includes a blend of these components, tailored to the specific task at hand. Each element contributes to shaping the model’s output, guiding it towards generating responses that align with the desired goal.

Contact LeewayHertz’s prompt engineers today!
Enhance the performance of your AI models with our prompt engineering services

Learn more
How to design prompts?
Importance of LLM settings
Designing prompts for a large language model involves understanding and manipulating specific settings that can steer the model’s output. These settings can be modified either directly or via an API.

Key settings include the ‘Temperature’ and ‘Top_p’ parameters. The ‘Temperature’ parameter controls the randomness of the model’s output. Lower values make the model’s output more deterministic, favoring the most probable next token. This is useful for tasks requiring precise and factual answers, like a fact-based question-answer system. On the other hand, increasing the ‘Temperature’ value induces more randomness in the model’s responses, allowing for more creative and diverse results. This is beneficial for creative tasks like poem generation.

The ‘Top_p’ parameter, used in a sampling technique known as nucleus sampling, also influences the determinism of the model’s response. A lower ‘Top_p’ value results in more exact and factual answers, while a higher value increases the diversity of the responses.

One key recommendation is to adjust either ‘Temperature’ or ‘Top_p,’ but not both simultaneously, to prevent overcomplicating the system and to better control the effect of these settings.

Remember that the performance of your prompt may vary depending on the version of LLM you are using, and it’s always beneficial to iterate and experiment with your settings and prompt design.

Key strategies for successful prompt design
Here are some tips to keep in mind while you are designing your prompts

Begin with the basics
While embarking on the journey of designing prompts you need to remember that it’s a step-by-step process that demands persistent tweaking and testing to achieve excellence. Platforms like OpenAI or Cohere provide a user-friendly environment for this venture. Kick off with basic prompts, gradually enriching them with more components and context as you strive for enhanced outcomes. Maintaining different versions of your prompts is crucial in this progression. Through this guide, you will discover that clarity, simplicity, and precision often lead to superior results.

For complex tasks involving numerous subtasks, consider deconstructing them into simpler components, progressively developing as you achieve promising results. This approach prevents an overwhelming start to the prompt design process.

Crafting effective prompts: The power of instructions
As a prompt designer, one of your most potent tools is the instruction you give to the language model. Instructions such as “Write,” “Classify,” “Summarize,” “Translate,” “Order,” etc., guide the model to execute a variety of tasks.

Remember, crafting an effective instruction often involves a considerable amount of experimentation. To optimize the instruction for your specific use case, test different instruction patterns with varying keywords, contexts, and data types. The rule of thumb here is to ensure the context is as specific and relevant to your task as possible.

Here is a practical tip: most prompt designers suggest placing the instruction at the start of the prompt. A clear separator, like “###”, could be used to distinguish the instruction from the context. For example:

“### Instruction ### Translate the following text to French:

Text: “Good morning!” By following these guidelines, you will be well on your way to creating effective and precise prompts.

The essence of specificity in prompt design
In the realm of prompt design, specificity is vital. The more accurately you define the task and instruction, the more aligned the outcomes will be with your expectations. It’s not so much about using certain tokens or keywords, but rather about formulating a well-structured and descriptive prompt.

A useful technique is to include examples within your prompts; they can guide the model to produce the output in the desired format. For instance, if you are seeking a summarization of a text in three sentences, your instruction could be:

“Summarize the following text into 3 sentences: …”

Keep in mind that while specificity is important, there is a balance to be found. You should be conscious of the prompt’s length, as there are limitations to consider. Additionally, overloading the prompt with irrelevant details may confuse the model rather than guiding it. The goal is to include details that meaningfully contribute to the task at hand.

Prompt design is a process of constant experimentation and iteration. Always seek to refine and enhance your prompts for optimal outcomes. Experiment with different levels of specificity and detail to find what works best for your unique applications.

Sidestepping ambiguity in prompt design
While prompt design requires a balance of detail and creativity, it is crucial to avoid ambiguity or impreciseness. Much like clear communication, precise instructions yield better results. An overly clever or convoluted prompt can lead to less desirable outcomes. Instead, focus on clarity and specificity.

For instance, let’s say you want your model to generate a brief definition of the term ‘Artificial Intelligence’. An imprecise prompt might be:

“Talk about this thing that’s being used a lot these days, Artificial Intelligence.”

While the model may understand this prompt, it’s indirect and lacks clarity. You may receive a lengthy discourse rather than the succinct definition you desire. A clearer, more direct prompt could be:

“Define the term ‘Artificial Intelligence’ in one sentence.”

This prompt is precise and directs the model to generate a specific output. The output, in this case, could be:

“Artificial Intelligence is a branch of computer science focused on creating machines capable of mimicking human intelligence.”

Through avoiding ambiguity in your prompts, you can effectively guide the model to produce the desired output.

Choosing clarity over restrictions
In prompt designing, it’s often more beneficial to instruct the model on what to do, rather than dictating what not to do. This approach promotes precision and directs the model towards generating useful responses.

Consider, for instance, you are trying to get a language model to recommend a book. An ineffective instruction might be:

“Do not recommend any books that are not in the top 10 best sellers list.”

This statement might confuse the model as it’s primarily focusing on what not to do. A more productive instruction would be:

“Recommend a book from the top 10 best sellers list.”

This is direct, clear, and focuses the model on the task you desire. The output could be:

“One of the top 10 best sellers right now is ‘The Code Breaker’ by Walter Isaacson. It’s a fascinating look into the world of genetics and biochemistry.”

Emphasizing the desired action in your prompt, rather than the prohibited ones, ensures the model clearly understands your expectations and is more likely to deliver an appropriate response.

A case study on importance of effective prompt
To illustrate the significance of a carefully composed prompt, let’s say we are developing an XGBoost model and our goal is to author a Python script that carries out hyperparameter optimization. The data we are working with is voluminous and not evenly distributed.

We are going to experiment with two distinct prompts both intended to generate code aiding hyperparameter optimization. The first prompt offers just the basic context, whereas the second one is enhanced with some supplementary directives.

Prompt 1: Write Python code that executes hyperparameter tuning on an XGBoost classifier.

Here is the code that ChatGPT generates (model used here in GPT4):

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from xgboost import XGBClassifier
 
# Load dataset
data = pd.read_csv('PATH_TO_YOUR_DATA')
 
# Assume 'target' is your target variable
X = data.drop('target', axis=1)
y = data['target']
 
# Split dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test
 
# Define XGBoost classifier
xgb = XGBClassifier()
 
# Define hyperparameter grid
param_grid = {
    'n_estimators': [100, 200, 500],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 4, 5],
    'colsample_bytree': [0.3, 0.7]
}
 
# Initialize GridSearchCV
grid = GridSearchCV(xgb, param_grid, cv=5)
 
# Fit GridSearchCV
grid.fit(X_train, y_train)
 
# Print best parameters
print("Best Parameters: ", grid.best_params_)
 
# Print best score
print("Best Score: ", grid.best_score_)
The code uses Scikit-learn’s GridSearchCV for hyperparameter tuning in an XGBoost classifier. Please replace “PATH_TO_YOUR_DATA” with the actual path to your dataset and ensure that your target variable is properly defined. Also, adjust the parameters as per your requirements.

The solution provided does function as expected, but it may not perform optimally for larger datasets or those with imbalanced classes. The grid search approach, while thorough, can be both inefficient and time-consuming. Moreover, using accuracy as a metric can be misleading when dealing with imbalanced data, often giving a false sense of model performance.

In essence, this underlines how a lack of sufficient information in a prompt can lead to less-than-ideal solutions.

Now, let’s enhance our prompt by incorporating additional instructions and observe how it impacts the resulting output.

Prompt 2: Write Python code that executes hyperparameter tuning on an XGBoost classifier. Carry out 50 trials using the Optuna library, with f-1 score as the evaluation metric. Only consider the following hyperparameters: n_estimators, learning_rate, gamma, and max_depth.

Here is the generated code

import optuna
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
 
# Assuming X and y are your features and target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
 
def objective(trial):
    n_estimators = trial.suggest_int("n_estimators", 2, 150)
    learning_rate = trial.suggest_loguniform("learning_rate", 1e-5, 1e-2)
    gamma = trial.suggest_loguniform("gamma", 1e-8, 1.0)
    max_depth = trial.suggest_int("max_depth", 1, 9)
     
    classifier_obj = XGBClassifier(
        n_estimators=n_estimators,
        learning_rate=learning_rate,
        gamma=gamma,
        max_depth=max_depth,
        objective="binary:logistic")
     
    classifier_obj.fit(X_train, y_train)
     
    y_pred = classifier_obj.predict(X_test)
    return f1_score(y_test, y_pred)
 
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=50)
 
trial = study.best_trial
 
print(f"Best Trial: score {trial.value}, params {trial.params}")
In this revised prompt, there’s less ambiguity, with clear indications about the library to use, which hyperparameters to adjust, how many trials to conduct, and the evaluation metric to employ. Just two extra sentences compared to the original prompt, yet it delivers a significantly better code output!

As observed, the code generated by ChatGPT makes use of the Optuna library for Bayesian search on the specified four hyperparameters, utilizing the f1-score as the evaluation measure. This approach is far more efficient and less time-intensive than the one proposed in response to the earlier prompt.

Prompt engineering best practices
Craft detailed and direct instructions
Strategy 1: Use delimiters such as , “““, < >, <tag> </tag> to distinguish different sections of the input. This helps in structuring your input effectively and preventing prompt errors. For instance, using the delimiters to specify the text to be summarized.
Strategy 2: Request for a structured output. This could be in a JSON format, which can easily be converted into a list or dictionary in Python later on.
Strategy 3: Confirm whether conditions are met. The prompt can be designed to verify assumptions first. This is particularly helpful when dealing with edge cases. For example, if the input text doesn’t contain any instructions, you can instruct the model to write “No steps provided”.
Strategy 4: Leverage few-shot prompting. Provide the model with successful examples of completed tasks, then ask the model to carry out a similar task.
Allow the model time to ‘Think’
Strategy 1: Detail the steps needed to complete a task and demand output in a specified format. For complex tasks, breaking them down into smaller steps can be beneficial, just as humans often find step-by-step instructions helpful. You can ask the model to follow a logical sequence or chain of reasoning before arriving at the final answer.
Strategy 2: Instruct the model to work out its solution before jumping to a conclusion. This helps the model in thoroughly processing the task at hand before delivering the output.
Opt for the latest model
To attain optimal results, it is advisable to use the most advanced models.

Provide detailed descriptions
Clarity is crucial. Be specific and descriptive about the required context, outcome, length, format, style, etc. For instance, instead of simply requesting a poem about OpenAI, specify details like poem length, style, and a particular theme, such as a recent product launch.

Use examples to illustrate desired output format
The model responds better to specific format requirements shown through examples. This approach also simplifies the process of parsing multiple outputs programmatically.

Start with zero-shot, then few-shot, and finally fine-tune
For complex tasks, start with zero-shot, then proceed with few-shot techniques. If these methods don’t yield satisfactory results, consider fine-tuning the model.

Eliminate vague and unnecessary descriptions
Precision is essential. Avoid vague and “fluffy” descriptions. For instance, instead of saying, “The description should be fairly short,” provide a clear guideline such as, “Use a 3 to 5 sentence paragraph to describe this product.”

Give direct instructions over prohibitions
Instead of telling the model what not to do, instruct it on what to do. For instance, in a customer service conversation scenario, instruct the model to diagnose the problem and suggest a solution, avoiding any questions related to personally identifiable information (PII).

Use leading words for code generation
For code generation tasks, nudge the model towards a particular pattern using leading words. This might include using words like ‘import’ to hint the model that it should start writing in Python, or ‘SELECT’ for initiating a SQL statement.

Contact LeewayHertz’s prompt engineers today!
Enhance the performance of your AI models with our prompt engineering services

Learn more
Applications of prompt engineering
Program-aided Language Model (PAL)
Program-aided language models in prompt engineering involve integrating programmatic instructions and structures to enhance the capabilities of language models. By incorporating additional programming logic and constraints, PAL enables more precise and context-aware responses. This approach allows developers to guide the model’s behavior, specify the desired output format, provide relevant examples, and refine prompts based on intermediate results. By leveraging programmatic guidance, PAL techniques empower language models to generate more accurate and tailored responses, making them valuable tools for a wide range of applications in natural language processing.

Here is an example of how PAL can be applied in prompt engineering:

Prompt:

Given a list of numbers, compute the sum of all even numbers.
Input: [2, 5, 8, 10, 3, 6]
Output: The sum of all even numbers is 26.
In this example, the prompt includes a programmatic instruction to compute the sum of even numbers in a given list. By providing this specific task and format, the language model guided by PAL techniques can generate a response that precisely fulfills the desired computation. The integration of programmatic logic and instructions in the prompt ensures accurate and contextually appropriate results.

Generating data
Generating data is an important application of prompt engineering with large language models (LLMs). LLMs have the ability to generate coherent and contextually relevant text, which can be leveraged to create synthetic data for various purposes.

For example, in natural language processing tasks, generating data using LLMs can be valuable for training and evaluating models. By designing prompts that instruct the LLM to generate specific types of data, such as question-answer pairs, text summaries, or dialogue interactions, researchers and practitioners can create large volumes of labeled training data. This synthetic data can then be used to train and improve NLP models, as well as to evaluate their performance.

Here is an example:

Prompt:

Generate 100 question-answer pairs about famous landmarks.
Using this prompt, the LLM can generate a diverse set of question-answer pairs related to famous landmarks around the world. The generated data can be used to enhance question-answering models or to augment existing datasets for training and evaluation.

By employing prompt engineering techniques, researchers and developers can effectively utilize LLMs to generate data that aligns with their specific needs, enabling them to conduct experiments, evaluate models, and advance various domains of research.

Generating code
Generating code is another application of prompt engineering with large language models. LLMs can be prompted to generate code snippets, functions, or even entire programs, which can be valuable in software development, automation, and programming education.

For example, let’s consider a scenario where a developer wants to generate a Python function that calculates the factorial of a number:

Prompt:

Write a Python function named "factorial" that takes an integer as input and returns its factorial.
By providing this specific prompt to the LLM, it can generate code that implements the factorial function in Python:

Generated Code:

def factorial(n):
    if n == 0 or n == 1:
        return 1
    else:
        return n * factorial(n - 1)
The generated code demonstrates the recursive implementation of the factorial function in Python.

Prompt engineering allows developers to design prompts with clear instructions and specifications, such as function names, input requirements, and desired output formats. By carefully crafting prompts, LLMs can be guided to generate code snippets tailored to specific programming tasks or requirements.

This application of prompt engineering can be highly beneficial for developers seeking assistance in code generation, automating repetitive tasks, or even for educational purposes where learners can explore different code patterns and learn from the generated examples.

Risks associated with prompting and solutions
As we harness the power of large language models and explore their capabilities, it is important to acknowledge the risks and potential misuses associated with prompting. While well-crafted prompts can yield impressive results, it is crucial to understand the potential pitfalls and safety considerations when using LLMs for real-world applications.

This section sheds light on the risks and misuses of LLMs, particularly through techniques like prompt injections. It also addresses harmful behaviors that may arise and provides insights into mitigating these risks through effective prompting techniques. Additionally, topics such as generalizability, calibration, biases, social biases, and factuality are explored to foster a comprehensive understanding of the challenges involved in working with LLMs.

By recognizing these risks and adopting responsible practices, we can navigate the evolving landscape of LLM applications while promoting ethical and safe use of these powerful language models.

Adversarial prompting
Adversarial prompting refers to the intentional manipulation of prompts to exploit vulnerabilities or biases in language models, resulting in unintended or harmful outputs. Adversarial prompts aim to trick or deceive the model into generating misleading, biased, or inappropriate responses.

Prompt injection: Prompt injection is a technique used in adversarial prompting where additional instructions or content is inserted into the prompt to influence the model’s behavior. By injecting specific keywords, phrases, or instructions, the model’s output can be manipulated to produce desired or undesired outcomes. Prompt injection can be used to introduce biases, generate offensive or harmful content, or manipulate the model’s understanding of the task.
Prompt leaking: Prompt leaking occurs when sensitive or confidential information unintentionally gets exposed in the model’s response. This can happen when the model incorporates parts of the prompt, including personally identifiable information, into its generated output. Prompt leaking poses privacy and security risks, as it may disclose sensitive data to unintended recipients or expose vulnerabilities in the model’s handling of input prompts.
Jailbreaking: In the context of prompt engineering, jailbreaking refers to bypassing or overriding safety mechanisms put in place to restrict or regulate the behavior of language models. It involves manipulating the prompt in a way that allows the model to generate outputs that may be inappropriate, unethical, or against the intended guidelines. Jailbreaking can lead to the generation of offensive content, misinformation, or other undesirable outcomes.
Overall, adversarial prompting techniques like prompt injection, prompt leaking, and jailbreaking highlight the importance of responsible and ethical prompt engineering practices. It is essential to be aware of the potential risks and vulnerabilities associated with language models and to take precautions to mitigate these risks while ensuring the safe and responsible use of these powerful AI systems.

Defense tactics for adversarial prompting
Add defense in the instruction: One defense tactic is to explicitly enforce the desired behavior through the instruction given to the model. While this approach is not foolproof, it emphasizes the power of well-crafted prompts in guiding the model towards the intended output.
Parameterize prompt components: Inspired by techniques used in SQL injection, one potential solution is to parameterize different components of the prompt, separating instructions from inputs and handling them differently. This approach can lead to cleaner and safer solutions, although it may come with some trade-offs in terms of flexibility.
Quotes and additional formatting: Escaping or quoting input strings can provide a workaround to prevent certain prompt injections. This tactic, suggested by Riley, helps maintain robustness across phrasing variations and highlights the importance of proper formatting and careful consideration of prompt structure.
Adversarial prompt detector: Language models themselves can be leveraged to detect and filter out adversarial prompts. By fine-tuning or training an LLM specifically for detecting such prompts, it is possible to incorporate an additional layer of defense to mitigate the impact of adversarial inputs.
Selecting model types: Choosing the appropriate model type can also contribute to defense against prompt injections. For certain tasks, using fine-tuned models or creating k-shot prompts for non-instruct models can be effective. Fine-tuning a model on a large number of examples can help improve robustness and accuracy, reducing reliance on instruction-based models.
Guardrails and safety measures: Some language models, like ChatGPT, incorporate guardrails and safety measures to prevent malicious or dangerous prompts. While these measures provide a level of protection, they are not perfect and can still be susceptible to novel adversarial prompts. It is important to recognize the trade-off between safety constraints and desired behaviors.
Factuality
It is worth noting that the field of prompt engineering and defense against adversarial prompting is an evolving area, and more research and development are needed to establish robust and comprehensive defense tactics against text-based attacks. Factuality is a significant risk in prompting as LLMs can generate responses that appear coherent and convincing but may lack accuracy. To address this, there are several solutions that can be employed:

Provide ground truth: Including reliable and factual information as part of the context can help guide the model to generate more accurate responses. This can involve referencing related articles, excerpts from reliable sources, or specific sections from Wikipedia entries. By incorporating verified information, the model is less likely to produce fabricated or inconsistent responses.
Control response diversity: Modifying the probability parameters of the model can influence the diversity of its responses. By decreasing the probability values, the model can be guided towards generating more focused and factually accurate answers. Additionally, explicitly instructing the model to acknowledge uncertainty by admitting when it doesn’t possess the required knowledge can also mitigate the risk of generating false information.
Provide examples in the prompt: Including a combination of questions and responses in the prompt can guide the model to differentiate between topics it is familiar with and those it is not. By explicitly demonstrating examples of both known and unknown information, the model can better understand the boundaries of its knowledge and avoid generating false or speculative responses.
These solutions help address the risk of factuality in prompting by promoting more accurate and reliable output from LLMs. However, it is important to continuously evaluate and refine the prompt engineering strategies to ensure the best possible balance between generating coherent responses and maintaining factual accuracy.

Biases
Biases in LLMs pose a significant risk as they can lead to the generation of problematic and biased content. These biases can adversely impact the performance of the model in downstream tasks and perpetuate harmful stereotypes or discriminatory behavior. To address this, it is essential to implement appropriate solutions:

Effective prompting strategies: Crafting well-designed prompts can help mitigate biases to some extent. By providing specific instructions and context that encourage fairness and inclusivity, the model can be guided to generate more unbiased responses. Additionally, incorporating diverse and representative examples in the prompt can help the model learn from a broader range of perspectives, reducing the likelihood of biased output.
Moderation and filtering: Implementing robust moderation and filtering mechanisms can help identify and mitigate biased content generated by LLMs. This involves developing systems that can detect and flag potentially biased or harmful outputs in real-time. Human reviewers or content moderation teams can then review and address any problematic content, ensuring that biased or discriminatory responses are not propagated.
Diverse training data: Training LLMs on diverse datasets that encompass a wide range of perspectives and experiences can help reduce biases. By exposing the model to a more comprehensive set of examples, it learns to generate responses that are more balanced and representative. Regularly updating and expanding the training data with diverse sources can further enhance the model’s ability to generate unbiased content.
Post-processing and debiasing techniques: Applying post-processing techniques to the generated output can help identify and mitigate biases. These techniques involve analyzing the model’s responses for potential biases and adjusting them to ensure fairness and inclusivity. Debiasing methods can be employed to retrain the model, explicitly addressing and reducing biases in its output.
It is important to note that addressing biases in LLMs is an ongoing challenge, and no single solution can completely eliminate biases. It requires a combination of thoughtful prompt engineering, robust moderation practices, diverse training data, and continuous improvement of the underlying models. Close collaboration between researchers, practitioners, and communities is crucial to develop effective strategies and ensure responsible and unbiased use of LLMs.
