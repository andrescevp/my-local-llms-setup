# Local LLMs Stack

## Requirements

Install Ollama locally to better performance: https://ollama.com/download
If you want to have it in docker: https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image

## Install

```bash
python build_env.py
```

# N8N Community Nodes installed:

- `n8n-nodes-browserless`: Requires Browserless

